require 'bundler'
Bundler.require

require 'sinatra'
require 'sinatra/streaming'
require_relative 'model_mock'

Faye::WebSocket.load_adapter('thin')

set :port, ENV['PORT'] || 3100
set :bind, '0.0.0.0'
set :title, ENV['TITLE'] || 'LLama 2 Private LLM Frontend'
set :test_mode, ENV['TEST_MODE'] || false

# location of the quantized model bin file generated by llama.cpp
MODEL = ENV["MODEL_PATH"] || "/data/models/model.bin"


def create_model_and_context
  if settings.test_mode
    model = FakeModel.new
    context = FakeContext.new(model: model)
    [model, context]
  else
    params = LLaMACpp::ContextParams.new
    params.seed = 42
    model = LLaMACpp::Model.new(model_path: MODEL, params: params)
    context = LLaMACpp::Context.new(model: model)
    [model, context]
  end
end

model, context = create_model_and_context

def build_prompt(instruction, input)
  "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. You may use markdown format and express your response\n" +
  "### Instruction:\n" + instruction + "\n### Input:\n" + input + "\n### Response:\n"
end

set :sockets, []
set :n_threads, (ENV['N_THREADS'] || 8).to_i

get '/' do
  @server_name = ENV['SERVER_NAME'] || 'localhost'
  erb :chat, { :locals => { :title => settings.title, :host_name => @server_name, :port => settings.port } }
end

get '/chat' do
  @server_name = ENV['SERVER_NAME'] || 'localhost'
  if Faye::WebSocket.websocket?(request.env)
    ws = Faye::WebSocket.new(request.env)

    ws.on :open do |event|
      logger.info("open #{ws}")
      settings.sockets << ws
      ws.send({type: 'info', message: ''}.to_json)
    end

    ws.on :message do |event|
      data = JSON.parse(event.data)
      case data['type']
      when 'ping'
        ws.send({type: 'ping', message: 'pong'}.to_json)
      when 'generate'
        query = data['message']
        n_predict = (data['n_predict'] || 128).to_i
        logger.info(query)
        Thread.new do
          output = LLaMACpp.generate(context, query, n_threads: settings.n_threads, n_predict: n_predict, streaming_callback: ->(token) {
            ws.send({type: 'stream', user: data['user'], message: token}.to_json)
          })
          logger.info("response: [#{output}]...")
          ws.send({type: 'message', user: data['user'], message: output}.to_json)
        end
      when 'message'
        query = build_prompt(data['context'], data['message'])
        n_predict = (data['n_predict'] || 128).to_i
        logger.info(query)

        # Move long-running task to a separate thread
        Thread.new do
          output = LLaMACpp.generate(context, query, n_threads: settings.n_threads, n_predict: n_predict, streaming_callback: ->(token) {
            ws.send({type: 'stream', user: data['user'], message: token}.to_json)
          })
          logger.info("response: [#{output}]...")
          matches = output.scan(/### Response:\n(.*?)(?=###|\z)/m)
          message =  matches[0]
          logger.info("response: [#{message[0]}]...")
          ws.send({type: 'message', user: data['user'], message: message[0]}.to_json)
        end
      else
        ws.send({type: 'error', message: 'Unknown command!'}.to_json)
      end
    end

    ws.on :close do |event|
      logger.info("close #{ws}")
      settings.sockets.delete(ws)
    end

    ws.rack_response
  else
    
    erb :chat, { :locals => { :title => settings.title, :host_name => @server_name, :port => settings.port } }
  end
end

post '/v1/completions' do
  # Read and validate incoming JSON payload
  begin
    payload = JSON.parse(request.body.read)
    prompt = payload["prompt"]
    max_tokens = payload["max_tokens"] || 100
    model_name = payload["model"] || "default"
    temperature = payload["temperature"] || 1.0
    stream = payload["stream"] || false
  rescue JSON::ParserError => e
    halt 400, { message: "Invalid JSON" }.to_json
  end

  # Generate text using LLaMACpp model
  if stream
    content_type "text/event-stream"

    stream(:keep_open) do |out|
      begin
        LLaMACpp.generate(context, prompt, n_threads: settings.n_threads, n_predict: max_tokens, streaming_callback: ->(token) {
          out << "data: #{token}\n\n"
        })
        out << "data: [DONE]\n\n"
      rescue => e
        out << "data: [ERROR]\n\n"
      ensure
        out.close
      end
    end
  else
    output = nil
    begin
      output = LLaMACpp.generate(context, prompt, n_threads: settings.n_threads, n_predict: max_tokens)
    rescue => e
      halt 500, { message: "Error in generation" }.to_json
    end

    # Generate a response in the specified format
    response = {
      "id" => "cmpl-" + SecureRandom.hex(8),
      "object" => "text_completion",
      "created" => Time.now.to_i,
      "model" => model_name,
      "choices" => [
        {
          "text" => output,
          "index" => 0,
          "logprobs" => nil,
          "finish_reason" => "length"
        }
      ],
      "usage" => {
        "prompt_tokens" => 5, # This should be calculated
        "completion_tokens" => max_tokens,
        "total_tokens" => 5 + max_tokens # This should be calculated
      }
    }

    content_type :json
    response.to_json
  end
end
