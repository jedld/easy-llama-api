require 'bundler'
Bundler.require

require 'sinatra'

Faye::WebSocket.load_adapter('thin')

set :port, 3100

# location of the quantized model bin file generated by llama.cpp
MODEL = ENV["MODEL_PATH"] || "ggml-model-q4_0.bin"

params = LLaMACpp::ContextParams.new
params.seed = 42
model = LLaMACpp::Model.new(model_path: MODEL, params: params)
context = LLaMACpp::Context.new(model: model)
LLaMACpp.generate(context, "Hello", n_threads: 4)

def build_prompt(instruction, input)
  "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n" +
  "### Instruction:\n" + instruction + "\n### Input:\n" + input + "\n### Response:\n"
end

set :sockets, []

get '/' do
  erb :index
end

get '/chat' do
  if Faye::WebSocket.websocket?(request.env)
    ws = Faye::WebSocket.new(request.env)

    ws.on :open do |event|
      settings.sockets << ws
      ws.send({type: 'info', message: ''}.to_json)
    end

    ws.on :message do |event|
      data = JSON.parse(event.data)
      case data['type']
      when 'message'
        query = build_prompt(data['context'], data['message'])
        logger.info(query)
        output = LLaMACpp.generate(context, query, n_threads: 4)
        logger.info("response: [#{output}]")
        settings.sockets.each{|s| s.send({type: 'message', user: data['user'], message: output}.to_json)}
      else
        ws.send({type: 'error', message: 'Unknown command!'}.to_json)
      end
    end

    ws.on :close do |event|
      settings.sockets.delete(ws)
    end

    ws.rack_response
  else
    erb :chat
  end
end